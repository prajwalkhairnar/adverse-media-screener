# =============================================================================
# Adverse Media Screening System - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your API keys
# At least one LLM provider API key is required

# -----------------------------------------------------------------------------
# LLM Provider Configuration
# -----------------------------------------------------------------------------
# Choose your default provider: groq, openai, or anthropic
DEFAULT_LLM_PROVIDER=groq

# API Keys - Provide at least one
# Get Groq key from: https://console.groq.com/keys
GROQ_API_KEY=

# Get OpenAI key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Get Anthropic key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=

# -----------------------------------------------------------------------------
# Model Selection (per provider)
# -----------------------------------------------------------------------------
# Groq models: llama-3.3-70b-versatile, llama-3.1-70b-versatile, mixtral-8x7b-32768
GROQ_MODEL=llama-3.3-70b-versatile

# OpenAI models: gpt-4o-2024-11-20, gpt-4o, gpt-4o-mini
OPENAI_MODEL=gpt-4o-2024-11-20

# Anthropic models: claude-sonnet-4-20250514, claude-3-5-sonnet-20241022
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# -----------------------------------------------------------------------------
# LLM Behavior Configuration
# -----------------------------------------------------------------------------
# Temperature (0 = deterministic, recommended for compliance)
LLM_TEMPERATURE=0

# Maximum retries for failed LLM calls
MAX_RETRIES=3

# Request timeout in seconds
REQUEST_TIMEOUT=60

# Enable prompt caching (Anthropic only - saves ~90% on repeated content)
ENABLE_PROMPT_CACHING=true

# Enable provider fallback (try backup provider if primary fails)
ENABLE_FALLBACK=false

# -----------------------------------------------------------------------------
# Observability & Logging
# -----------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARN, ERROR
LOG_LEVEL=INFO

# Log format: json or text
LOG_FORMAT=json

# Save detailed logs to file
SAVE_LOGS=true
LOG_FILE=outputs/screening.log

# LangSmith for LLM observability (optional but recommended)
# Get key from: https://smith.langchain.com/settings
LANGSMITH_API_KEY=
LANGSMITH_PROJECT=adverse-media-screening
LANGSMITH_TRACING=false

# -----------------------------------------------------------------------------
# Article Fetching Configuration
# -----------------------------------------------------------------------------
# User agent for web requests
USER_AGENT=Mozilla/5.0 (compatible; AdverseMediaScreener/1.0)

# Request timeout for article fetching (seconds)
ARTICLE_FETCH_TIMEOUT=30

# Maximum article length to process (characters)
MAX_ARTICLE_LENGTH=50000

# -----------------------------------------------------------------------------
# Matching Configuration
# -----------------------------------------------------------------------------
# Age tolerance for matching (years)
AGE_TOLERANCE=2

# Minimum match probability to consider as potential match (0.0 to 1.0)
MIN_MATCH_PROBABILITY=0.4

# Threshold for high confidence match
HIGH_CONFIDENCE_THRESHOLD=0.8

# Threshold for medium confidence match
MEDIUM_CONFIDENCE_THRESHOLD=0.6

# -----------------------------------------------------------------------------
# Performance & Cost Optimization
# -----------------------------------------------------------------------------
# Enable result caching (not implemented in MVP)
ENABLE_CACHING=false

# Maximum concurrent LLM requests
MAX_CONCURRENT_REQUESTS=3

# Cost alert threshold (USD)
COST_ALERT_THRESHOLD=1.0